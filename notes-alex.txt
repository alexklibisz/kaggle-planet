7/3/17

- Godard-V2
    - Godard with sigmoid outputs seems to improve when you decrease depth. Using 3 conv/prelu/conv/prelu/pooling blocks and one 512-dense prelu layer ends up plateauing around F2=0.89, whereas doing 5 conv blocks and two 512-dense layers plateaus around F2=0.86. This might be indicative of the same phenomenon that motivated the Residual architecture.
    - Increasing the beta and gamma regularization on the last batch norm layer before the sigmoid layer creates a significant shift in the sigmoid outputs. With no regularization, the sigmoid outputs quickly reach modes at 0.0 and 1.0. With l2(0.001), they have modes at 0.1 and 0.9, with l2(0.01), they have modes at 0.3 and 0.7. In principle, this should help the network to continue learning longer by preventing the sigmoid layer from saturating and having 0 gradients.
    - Even with less extreme sigmoid outputs, I still haven't found a way around the very peaky, almost-zero gradients.
    - Increasing the number of filters in the convolutional layers seems to drastically increase the distribution of gradients. When trained with 1/10th of the data, it reaches F2=0.93 in fifteen epochs. val F2 lags way behind at 0.81, which shows it has the capacity to overfit.

- ResNet50
    - Trying ResNet50 again after watching the CS231n lecture where Karpathy explains the motivation for this model. I think it might help alleviate some of the tiny-gradient issues with Godard-Sigmoid.

- Software structure
    - Still very messy. It would probably be smarter to still keep models in their own modules and expose a single random_hyperparms() function that can be invoked from the model runner interface.